{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yZjC1hSAshnN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RxPYxYig1tHf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, d_model, attention_type='multihead', alignment_fn='scaled_dot', n_heads=1, window_size=5):\n",
    "        super(CustomAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.attention_type = attention_type\n",
    "        self.alignment_fn = alignment_fn\n",
    "        self.n_heads = n_heads\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Linear layers for query, key, value (used in most attention mechanisms)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        if attention_type == 'multihead':\n",
    "            self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
    "\n",
    "    def compute_alignment(self, query, key):\n",
    "        if self.alignment_fn == 'dot':\n",
    "            return torch.matmul(query, key.transpose(-2, -1))\n",
    "        elif self.alignment_fn == 'scaled_dot':\n",
    "            return torch.matmul(query, key.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        elif self.alignment_fn == 'additive':\n",
    "            score = torch.tanh(query.unsqueeze(-2) + key.unsqueeze(-3))\n",
    "            return score.sum(dim=-1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown alignment function: {self.alignment_fn}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        if self.attention_type == 'multihead':\n",
    "            return self.attention(query, key, value)[0]\n",
    "\n",
    "        elif self.attention_type == 'local':\n",
    "            batch_size, seq_len, _ = x.size()\n",
    "            outputs = []\n",
    "            for i in range(seq_len):\n",
    "                start = max(0, i - self.window_size // 2)\n",
    "                end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "                local_query = query[:, i:i+1, :]\n",
    "                local_key = key[:, start:end, :]\n",
    "                local_value = value[:, start:end, :]\n",
    "                alignment = self.compute_alignment(local_query, local_key)\n",
    "                weights = F.softmax(alignment, dim=-1)\n",
    "                outputs.append(torch.matmul(weights, local_value).squeeze(1))\n",
    "            return torch.stack(outputs, dim=1)\n",
    "\n",
    "        elif self.attention_type == 'global':\n",
    "            alignment = self.compute_alignment(query, key)\n",
    "            weights = F.softmax(alignment, dim=-1)\n",
    "            return torch.matmul(weights, value)\n",
    "\n",
    "        elif self.attention_type == 'kernelized':\n",
    "            query = F.elu(query) + 1\n",
    "            key = F.elu(key) + 1\n",
    "            alignment = torch.matmul(query, key.transpose(-2, -1))\n",
    "            weights = F.softmax(alignment, dim=-1)\n",
    "            return torch.matmul(weights, value)\n",
    "\n",
    "        elif self.attention_type == 'group_query':\n",
    "            # Group query logic\n",
    "            groups = query.chunk(self.n_heads, dim=1)\n",
    "            grouped_results = []\n",
    "            for g in groups:\n",
    "                alignment = self.compute_alignment(g, key)\n",
    "                weights = F.softmax(alignment, dim=-1)\n",
    "                grouped_results.append(torch.matmul(weights, value))\n",
    "            return torch.cat(grouped_results, dim=1)\n",
    "\n",
    "        elif self.attention_type == 'hierarchical':\n",
    "            # Hierarchical: assume hierarchical levels as chunks\n",
    "            chunks = x.chunk(4, dim=1)  # Dividing into hierarchical levels\n",
    "            hierarchical_outputs = []\n",
    "            for chunk in chunks:\n",
    "                query = self.query(chunk)\n",
    "                key = self.key(chunk)\n",
    "                value = self.value(chunk)\n",
    "                alignment = self.compute_alignment(query, key)\n",
    "                weights = F.softmax(alignment, dim=-1)\n",
    "                hierarchical_outputs.append(torch.matmul(weights, value))\n",
    "            return torch.cat(hierarchical_outputs, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attention type: {self.attention_type}\")\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_hidden_dim, dropout, attention_type, alignment_fn):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = CustomAttention(d_model, attention_type=attention_type, alignment_fn=alignment_fn, n_heads=n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, d_model)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, ff_hidden_dim, num_layers, num_classes, max_seq_len, dropout=0.1, attention_type='multihead', alignment_fn='scaled_dot'):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, ff_hidden_dim, dropout, attention_type, alignment_fn)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gAEtsmGh1t6K"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training Loop\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation Loop\n",
    "def evaluate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            # Get predictions (threshold at 0.5 for multi-label classification)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "    # Concatenate predictions and labels\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='micro')\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "    return avg_valid_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW0d-18y4cHy",
    "outputId": "171d82b9-f080-4fca-d36d-c5684b86ba46"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gourishanker/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /home/gourishanker/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/gourishanker/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import reuters\n",
    "from nltk import download\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Download Reuters dataset\n",
    "download('reuters')\n",
    "download('punkt')\n",
    "\n",
    "# Load Reuters Dataset\n",
    "docs = reuters.fileids()\n",
    "documents = [reuters.raw(doc_id) for doc_id in docs]\n",
    "labels = [reuters.categories(doc_id) for doc_id in docs]\n",
    "\n",
    "# Binarize the labels for multi-label classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels_binarized = mlb.fit_transform(labels)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    documents, labels_binarized, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Custom Dataset\n",
    "class ReutersDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        features = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': features['input_ids'].squeeze(0),\n",
    "            'attention_mask': features['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Prepare DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "pandas.DataFrame({\"documents\":documents,\"labels\":labels}).to_csv(\"reuters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(\"reuters.csv\" , index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"len\"] = df[\"documents\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"len\",ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5GzJVGxv86hV",
    "outputId": "fafa61d1-bda5-4b43-e459-83a8f9ae6055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with Attention: group_query, Alignment: dot\n",
      "Epoch 1, Training Loss: 0.2105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gourishanker/miniconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0850\n",
      "Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Epoch 2, Training Loss: 0.0682\n",
      "Validation Loss: 0.0548\n",
      "Accuracy: 0.2924, Precision: 0.9254, Recall: 0.2396, F1 Score: 0.3806\n",
      "Model saved!\n",
      "Epoch 3, Training Loss: 0.0513\n",
      "Validation Loss: 0.0453\n",
      "Accuracy: 0.2952, Precision: 0.9969, Recall: 0.2415, F1 Score: 0.3888\n",
      "Model saved!\n",
      "Epoch 4, Training Loss: 0.0435\n",
      "Validation Loss: 0.0399\n",
      "Accuracy: 0.5042, Precision: 0.9042, Recall: 0.4179, F1 Score: 0.5716\n",
      "Model saved!\n",
      "Epoch 5, Training Loss: 0.0396\n",
      "Validation Loss: 0.0371\n",
      "Accuracy: 0.5190, Precision: 0.9045, Recall: 0.4300, F1 Score: 0.5829\n",
      "Model saved!\n",
      "Epoch 6, Training Loss: 0.0370\n",
      "Validation Loss: 0.0349\n",
      "Accuracy: 0.5185, Precision: 0.9244, Recall: 0.4394, F1 Score: 0.5957\n",
      "Epoch 7, Training Loss: 0.0347\n",
      "Validation Loss: 0.0334\n",
      "Accuracy: 0.5167, Precision: 0.9168, Recall: 0.4504, F1 Score: 0.6041\n",
      "Epoch 8, Training Loss: 0.0324\n",
      "Validation Loss: 0.0312\n",
      "Accuracy: 0.5556, Precision: 0.9100, Recall: 0.5053, F1 Score: 0.6498\n",
      "Model saved!\n",
      "Epoch 9, Training Loss: 0.0302\n",
      "Validation Loss: 0.0295\n",
      "Accuracy: 0.5658, Precision: 0.9150, Recall: 0.5136, F1 Score: 0.6579\n",
      "Model saved!\n",
      "Epoch 10, Training Loss: 0.0282\n",
      "Validation Loss: 0.0276\n",
      "Accuracy: 0.5959, Precision: 0.8991, Recall: 0.5568, F1 Score: 0.6877\n",
      "Model saved!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "attention_types = [\n",
    "    #\"kernelized\",   # Kernelized Attention\n",
    "    #\"local\",        # Local Attention\n",
    "    #\"global\",       # Global Attention\n",
    "    #\"multihead\",    # Multihead Attention\n",
    "    \"group_query\",  # Group Query Attention\n",
    "\n",
    "   # \"hierarchical\"  # Hierarchical Attention\n",
    "]\n",
    "\n",
    "# List of available alignment functions\n",
    "alignment_functions = [\n",
    "    \"dot\",          # Dot Product Alignment\n",
    "   # \"scaled_dot\",   # Scaled Dot Product Alignment\n",
    "    #\"additive\"      # Additive (Bahdanau) Alignment\n",
    "]\n",
    "\n",
    "vocab_size = tokenizer.vocab_size+1\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "ff_hidden_dim = 512\n",
    "num_layers = 4\n",
    "num_classes =len(mlb.classes_)\n",
    "max_seq_len = 1024\n",
    "batch_size = 32\n",
    "device = 'cuda'\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "train_dataset = ReutersDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ReutersDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "# Example usage in a loop\n",
    "for attn_type in attention_types:\n",
    "    for align_fn in alignment_functions:\n",
    "        print(f\"Testing with Attention: {attn_type}, Alignment: {align_fn}\")\n",
    "        model = DocumentClassifier(\n",
    "            vocab_size,\n",
    "            d_model,\n",
    "            n_heads,\n",
    "            ff_hidden_dim,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            max_seq_len,\n",
    "            attention_type=attn_type,\n",
    "            alignment_fn=align_fn\n",
    "        )\n",
    "        model.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        accuracy = 0\n",
    "        for epoch in range(10):\n",
    "            train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "            print(f\"Epoch {epoch+1}, Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "            # Evaluate on validation data\n",
    "            avg_valid_loss, metrics = evaluate(model, valid_loader, criterion, device)\n",
    "            print(f\"Validation Loss: {avg_valid_loss:.4f}\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "            if(accuracy < metrics['accuracy']):\n",
    "                accuracy = metrics['accuracy']\n",
    "                torch.save(model.state_dict(), f\"best_model_{attn_type}_{align_fn}.pth\")\n",
    "                print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-v9n6kWE4edB"
   },
   "outputs": [],
   "source": [
    "x1 = torch.randn((32, 128 ,128))\n",
    "x2 = torch.randn((32, 128 ,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n",
      "torch.Size([32, 128, 16])\n"
     ]
    }
   ],
   "source": [
    "for x in x1.chunk(8, dim=-1):\n",
    "    print(x.shape) # Dividing into hierarchical levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [32, 128] but got: [32, 16].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [32, 128] but got: [32, 16]."
     ]
    }
   ],
   "source": [
    "x1 @ x2.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "name": "NLP from Scratch: Annotated Attention"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
